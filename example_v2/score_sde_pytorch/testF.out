WARNING:tensorflow:From /home/yifulu/.conda/envs/dpm/lib/python3.9/site-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.

W0209 21:13:59.824184 22999636850496 utils.py:10] No checkpoint found at experiments/dpm_fewer/checkpoints-meta/checkpoint.pth. Returned the same state as input
I0209 21:13:59.826240 22999636850496 xla_bridge.py:355] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0209 21:13:59.826445 22999636850496 xla_bridge.py:355] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0209 21:13:59.826522 22999636850496 xla_bridge.py:355] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
I0209 21:13:59.827310 22999636850496 xla_bridge.py:355] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
I0209 21:13:59.827423 22999636850496 xla_bridge.py:355] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
W0209 21:13:59.827509 22999636850496 xla_bridge.py:362] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
I0209 21:13:59.829478 22999636850496 dataset_info.py:358] Load dataset info from /home/yifulu/tensorflow_datasets/cifar10/3.0.2
W0209 21:13:59.831874 22999636850496 options.py:503] options.experimental_threading is deprecated. Use options.threading instead.
W0209 21:13:59.831986 22999636850496 options.py:503] options.experimental_threading is deprecated. Use options.threading instead.
I0209 21:13:59.832121 22999636850496 dataset_builder.py:351] Reusing dataset cifar10 (/home/yifulu/tensorflow_datasets/cifar10/3.0.2)
I0209 21:13:59.832224 22999636850496 logging_logger.py:35] Constructing tf.data.Dataset cifar10 for split train, from /home/yifulu/tensorflow_datasets/cifar10/3.0.2
W0209 21:14:00.006322 22999636850496 options.py:503] options.experimental_threading is deprecated. Use options.threading instead.
W0209 21:14:00.006561 22999636850496 options.py:503] options.experimental_threading is deprecated. Use options.threading instead.
I0209 21:14:00.006695 22999636850496 dataset_builder.py:351] Reusing dataset cifar10 (/home/yifulu/tensorflow_datasets/cifar10/3.0.2)
I0209 21:14:00.006782 22999636850496 logging_logger.py:35] Constructing tf.data.Dataset cifar10 for split test, from /home/yifulu/tensorflow_datasets/cifar10/3.0.2
I0209 21:14:00.226277 22999636850496 run_lib.py:123] Starting training loop at step 0.
fewer step set
fewer step set
Traceback (most recent call last):
  File "/gpfs/accounts/qingqu_root/qingqu1/yifulu/dpm-solver/example_v2/score_sde_pytorch/main.py", line 64, in <module>
    app.run(main)
  File "/home/yifulu/.conda/envs/dpm/lib/python3.9/site-packages/absl/app.py", line 300, in run
    _run_main(main, args)
  File "/home/yifulu/.conda/envs/dpm/lib/python3.9/site-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "/gpfs/accounts/qingqu_root/qingqu1/yifulu/dpm-solver/example_v2/score_sde_pytorch/main.py", line 55, in main
    run_lib.train(FLAGS.config, FLAGS.workdir)
  File "/gpfs/accounts/qingqu_root/qingqu1/yifulu/dpm-solver/example_v2/score_sde_pytorch/run_lib.py", line 131, in train
    loss = train_step_fn(state, batch)
  File "/gpfs/accounts/qingqu_root/qingqu1/yifulu/dpm-solver/example_v2/score_sde_pytorch/losses.py", line 209, in step_fn
    loss = loss_fn(model, batch)
  File "/gpfs/accounts/qingqu_root/qingqu1/yifulu/dpm-solver/example_v2/score_sde_pytorch/losses.py", line 96, in loss_fn
    t = lst_steps.to(batch.device)[torch.randint(lst_steps.shape[0],batch.shape[0])]
TypeError: randint(): argument 'size' (position 2) must be tuple of ints, not int
2023-02-09 21:14:02.229658: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
